{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6344cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client configuration for Azure OpenAI and Foundry\n",
    "import asyncio\n",
    "\n",
    "from agent_framework.azure import AzureAIAgentClient, AzureOpenAIChatClient\n",
    "from agent_framework.observability import setup_observability \n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.identity.aio import DefaultAzureCredential as AsyncDefaultAzureCredential\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import config\n",
    "\n",
    "load_dotenv()\n",
    "setup_observability()\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "async_credential = AsyncDefaultAzureCredential()    \n",
    "openai_client = AzureOpenAI(\n",
    "    azure_endpoint=config.azure_openai_endpoint,\n",
    "    azure_ad_token_provider=lambda: credential.get_token(\"https://cognitiveservices.azure.com/.default\").token,\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "chat_client = AzureOpenAIChatClient(\n",
    "    credential=credential,\n",
    "    deployment_name=\"gpt-5-chat\",\n",
    "    endpoint=config.azure_openai_endpoint\n",
    ")\n",
    "\n",
    "agent_client = AzureAIAgentClient(\n",
    "    project_endpoint=config.foundry_project_endpoint,\n",
    "    model_deployment_name=\"gpt-5-chat\",\n",
    "    async_credential=async_credential,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0fb8cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tool to search our knowledge base\n",
    "from pydantic import Field\n",
    "from typing import List, Annotated\n",
    "from search_knowledge_base import KnowledgeBaseSearcher\n",
    "from agent_framework import ContextProvider\n",
    "\n",
    "def search_knowledge_base(\n",
    "        query: Annotated[str, Field(description=\"The search query string.\")]\n",
    "    ) -> List[str]:\n",
    "    \"\"\"Search the knowledge base for relevant information.\"\"\"\n",
    "\n",
    "    searcher = KnowledgeBaseSearcher()\n",
    "    results = searcher.semantic_search(query)\n",
    "\n",
    "    return [res[\"chunk\"] for res in results] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c871e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_instructions = \"\"\"\n",
    "You are a helpful AI assistant. You have access to a knowledge base about Meridian Strategic Consulting. \n",
    "Use the `search_knowledge_base` function to find relevant information from the knowledge base to answer user queries.\n",
    "Your tone should be friendly, professional and focussed on informing the user with accurate information.\n",
    "\"\"\"\n",
    "\n",
    "agent = agent_client.create_agent(\n",
    "    name=\"KnowledgeBaseAgent\",\n",
    "    instructions=agent_instructions,\n",
    "    tools=[search_knowledge_base]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2542045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for semantic similarity evaluation \n",
    "from typing import Any\n",
    "\n",
    "evaluation_instructions = \"\"\"\n",
    "You are an expert evaluator of AI generated answers.\n",
    "Given a reference answer, and an agent-generated response, evaluate the quality of the agent's response based on the following metrics. \n",
    "Provide a score from 1 to 5 for each metric, where 1 is poor and 5 is excellent. Also, provide an overall score with justification.\n",
    "\n",
    "REFERENCE ANSWER:\n",
    "{reference_answer}\n",
    "\n",
    "AGENT RESPONSE:\n",
    "{agent_response}\n",
    "\n",
    "EVALUATION METRICS:\n",
    "1. Accuracy: Are the facts and figures correct?\n",
    "2. Completeness: Does it cover all key points mentioned in the reference answer?\n",
    "3. Clarity: Is the response clear and well-structured?\n",
    "4. Relevance: Does it directly answer the user's question?\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "You MUST return your evaluation as a JSON object with the following structure:\n",
    "{{\n",
    "    \"accuracy\": X,\n",
    "    \"completeness\": X,\n",
    "    \"clarity\": X,\n",
    "    \"relevance\": X,\n",
    "    \"overall\": X,\n",
    "    \"justification\": \"Detailed explanation of your evaluation\"\n",
    "}}\n",
    "\n",
    "Where X is a score from 1-5 (1=Poor, 2=Below Average, 3=Average, 4=Good, 5=Excellent).\n",
    "\"\"\"\n",
    "\n",
    "async def evaluate_answer(response: str, reference: str) -> Any:\n",
    "    prompt = evaluation_instructions.format(reference_answer=reference, agent_response=response)\n",
    "\n",
    "    evaluation = await chat_client.get_response(\n",
    "        [prompt],\n",
    "        model=\"o3-mini\",\n",
    "        temperature=0.0,\n",
    "        tool_choice=\"auto\")\n",
    "\n",
    "    return evaluation.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "036c1c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meridian Strategic Consulting was founded in 2018.\n",
      "\n",
      "Would you like more details about the founders, headquarters, or company history?\n",
      "{\n",
      "    \"accuracy\": 5,\n",
      "    \"completeness\": 5,\n",
      "    \"clarity\": 5,\n",
      "    \"relevance\": 5,\n",
      "    \"overall\": 5,\n",
      "    \"justification\": \"The agent's response exactly matches the factual content of the reference answer, correctly stating that Meridian Strategic Consulting was founded in 2018. It fully covers the key point from the reference answer without omitting any information. The statement is clear, concise, and well-structured. The additional offer to provide more details is relevant and does not detract from the direct answer to the question. Overall, the response is accurate, complete, clear, and directly addresses the query.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = \"When was Meridian founded?\"\n",
    "reference_answer = \"Meridian Strategic Consulting was founded in 2018\"\n",
    "\n",
    "response = await agent.run(query, model=\"gpt-5-mini\")\n",
    "evaluation = await evaluate_answer(response.text, reference_answer)\n",
    "\n",
    "print(response)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa61ac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"accuracy\": 4,\n",
      "    \"completeness\": 2,\n",
      "    \"clarity\": 5,\n",
      "    \"relevance\": 3,\n",
      "    \"overall\": 3,\n",
      "    \"justification\": \"The agent's response is largely accurate in terms of general facts about Xebia, such as its founding location, services, and global presence. However, it omits key elements from the reference answer, such as Xebia's emphasis on responsible AI, core values, human experience, and its role in enabling industry disruptions and sustainable competitive advantage. The structure and clarity of the response are excellent, with well-organized bullet points. In terms of relevance, the response provides a general profile rather than directly mirroring the thematic and value-driven focus of the reference answer, which reduces alignment with the intended message. Overall, while factually sound and clear, the response lacks completeness and full relevance to the original content.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is Xebia?\"\n",
    "reference_answer = \"At Xebia, AI is in our DNA. We lead with responsible AI to shape a future grounded in a deep commitment to the human experience, driven by our core values. We collaborate with our clients and across our partner ecosystem to enable bold industry disruptions, accelerate innovation, deliver operational excellence, and secure sustainable competitive advantage. Through our Consulting, Software Engineering, and Training expertise, we help businesses be AI-first and future-ready.\"\n",
    "\n",
    "response = await agent.run(query, model=\"gpt-5-mini\")\n",
    "evaluation = await evaluate_answer(response.text, reference_answer)\n",
    "\n",
    "print(evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluation-intro (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
