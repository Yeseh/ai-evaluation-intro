{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6344cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client configuration for Azure OpenAI and Foundry\n",
    "import asyncio\n",
    "\n",
    "from agent_framework.azure import AzureAIAgentClient, AzureOpenAIChatClient\n",
    "from agent_framework.observability import setup_observability \n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.identity.aio import DefaultAzureCredential as AsyncDefaultAzureCredential\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import config\n",
    "\n",
    "load_dotenv()\n",
    "setup_observability()\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "async_credential = AsyncDefaultAzureCredential()    \n",
    "openai_client = AzureOpenAI(\n",
    "    azure_endpoint=config.azure_openai_endpoint,\n",
    "    azure_ad_token_provider=lambda: credential.get_token(\"https://cognitiveservices.azure.com/.default\").token,\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "chat_client = AzureOpenAIChatClient(\n",
    "    credential=credential,\n",
    "    deployment_name=\"gpt-5-chat\",\n",
    "    endpoint=config.azure_openai_endpoint\n",
    ")\n",
    "\n",
    "agent_client = AzureAIAgentClient(\n",
    "    project_endpoint=config.foundry_project_endpoint,\n",
    "    model_deployment_name=\"gpt-5-chat\",\n",
    "    async_credential=async_credential,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ab97ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test questions and reference answers \n",
    "test_cases = [\n",
    "    {\n",
    "        \"category\": \"Company Information\",\n",
    "        \"query\": \"When was Meridian founded?\",\n",
    "        \"reference_answer\": \"Meridian Strategic Consulting was founded in 2018\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Service Offerings\",\n",
    "        \"query\": \"What specific AI and automation services does Meridian offer, and what kind of ROI can clients expect?\",\n",
    "        \"reference_answer\": \"\"\"Meridian offers comprehensive AI & Automation Implementation services including AI opportunity assessment, solution design and architecture, implementation and testing, and change management support. These engagements typically last 16-28 weeks with teams of 5-10 consultants. Clients can expect a typical ROI of 400-600%, making this one of our highest-value service offerings. Dr. Amanda Foster leads this practice with expertise in machine learning, having 12 patents in ML and automation, and the team has delivered $500M+ in client value through AI implementations.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Expert Identification\", \n",
    "        \"query\": \"I need a consultant with AI/ML expertise and healthcare industry experience for a 6-month project. Who would be the best match?\",\n",
    "        \"reference_answer\": \"\"\"Dr. Amanda Foster would be the ideal match for this project. She is a Senior Partner with expertise in AI/ML and leads both the Digital Transformation practice and Healthcare vertical. She has a PhD in Computer Science from Carnegie Mellon, holds 12 patents in machine learning and automation, and has led AI implementations generating $500M+ in client value. She has specific healthcare experience with projects including AI-powered diagnostic tools, predictive analytics for patient outcomes, and clinical decision support systems. However, she is currently available starting February 2025, so timeline coordination would be needed.\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2542045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for semantic similarity evaluation \n",
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "evaluation_instructions = \"\"\"\n",
    "You are an expert evaluator of AI generated answers.\n",
    "Given a reference answer, evaluation criteria, and an agent-generated response, evaluate the quality of the agent's response based on the following metrics. \n",
    "Provide a score from 1 to 5 for each metric, where 1 is poor and 5 is excellent. Also, provide an overall score with justification.\n",
    "\n",
    "REFERENCE ANSWER:\n",
    "{reference_answer}\n",
    "\n",
    "AGENT RESPONSE:\n",
    "{agent_response}\n",
    "\n",
    "EVALUATION METRICS:\n",
    "1. Accuracy: Are the facts and figures correct?\n",
    "2. Completeness: Does it cover all key points mentioned in the reference answer?\n",
    "3. Clarity: Is the response clear and well-structured?\n",
    "4. Relevance: Does it directly answer the user's question?\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "You MUST return your evaluation as a JSON object with the following structure:\n",
    "{{\n",
    "    \"accuracy\": X,\n",
    "    \"completeness\": X,\n",
    "    \"clarity\": X,\n",
    "    \"relevance\": X,\n",
    "    \"overall\": X,\n",
    "    \"justification\": \"Detailed explanation of your evaluation\"\n",
    "}}\n",
    "\n",
    "Where X is a score from 1-5 (1=Poor, 2=Below Average, 3=Average, 4=Good, 5=Excellent).\n",
    "\"\"\"\n",
    "\n",
    "class EvaluationResponse(BaseModel):\n",
    "    accuracy: int\n",
    "    completeness: int\n",
    "    clarity: int\n",
    "    relevance: int\n",
    "    overall: int\n",
    "    justification: str\n",
    "\n",
    "async def evaluate_answer(response: str, reference: str) -> Any:\n",
    "    prompt = evaluation_instructions.format(reference_answer=reference, agent_response=response)\n",
    "\n",
    "    evaluation = await chat_client.get_response(\n",
    "        [prompt],\n",
    "        model=\"o3-mini\",\n",
    "        temperature=0.0,\n",
    "        tool_choice=\"auto\")\n",
    "\n",
    "    return evaluation.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d27c15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "from main import MeridianKnowledgeBaseAgent\n",
    "\n",
    "agent_instructions = \"\"\"\n",
    "You are a helpful AI assistant. \n",
    "\n",
    "You will receive a question and exerpts of documents returned from a search service in the following format\n",
    "```\n",
    "Question: {question}\n",
    "Exerpts: {context}\n",
    "```\n",
    "\n",
    "Keep thinking untill you have a complete answer. If the question contains multiple parts, split the query into parts and be sure to answer all parts of the question.\n",
    "\n",
    "Use ONLY the provided knowledge base excerpts to answer the user's question as accurately as possible. \n",
    "Do not make assumptions or add information not contained in the excerpts.\n",
    "If you cannot answer (part of the question) state which part of the question you are unable to answer.\n",
    "\"\"\"\n",
    "\n",
    "agent = MeridianKnowledgeBaseAgent(agent_instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036c1c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"accuracy\": 5,\n",
      "    \"completeness\": 5,\n",
      "    \"clarity\": 5,\n",
      "    \"relevance\": 5,\n",
      "    \"overall\": 5,\n",
      "    \"justification\": \"The agent's response exactly matches the reference answer, with the founding year stated correctly. It covers the only key point in the reference answer, is clear and concise, and directly addresses the question without unnecessary information. Therefore, it scores full marks across all metrics.\"\n",
      "}\n",
      "{\n",
      "    \"accuracy\": 3,\n",
      "    \"completeness\": 3,\n",
      "    \"clarity\": 5,\n",
      "    \"relevance\": 4,\n",
      "    \"overall\": 3,\n",
      "    \"justification\": \"The agent's response is well-structured and clear, making it easy to read and understand (clarity=5). It is relevant to the topic, describing Meridian's AI and automation services and including ROI, duration, and team size (relevance=4). However, accuracy suffers because the agent introduces additional service categories (Predictive Models & Analytics Capabilities) with ROI figures (250–500%) and durations (12–20 weeks) that are not mentioned in the reference answer, and omits the specific leadership detail about Dr. Amanda Foster, her patents, and the $500M+ client value delivered. Completeness is also impacted because while the core services from the reference are covered, the agent fails to mention the leadership and achievement details, which are key points in the reference answer. Overall, the response is clear and relevant but deviates from the reference facts and omits important information, resulting in an overall score of 3.\"\n",
      "}\n",
      "{\n",
      "    \"accuracy\": 4,\n",
      "    \"completeness\": 4,\n",
      "    \"clarity\": 5,\n",
      "    \"relevance\": 5,\n",
      "    \"overall\": 4,\n",
      "    \"justification\": \"The agent's response is largely accurate, correctly identifying Dr. Amanda Foster as the ideal candidate and including most of the key facts from the reference answer, such as her AI/ML expertise, 12 patents, $500M+ in client value, healthcare leadership role, and availability starting February 2025. However, there are slight deviations: the agent adds details like 'EHR optimization' and 'population health management' that were not in the reference, and omits specific healthcare projects mentioned in the reference (AI-powered diagnostic tools, clinical decision support systems). Completeness is good but not perfect due to these omissions. Clarity is excellent, with a well-structured and easy-to-read format. Relevance is excellent, as the response directly addresses the user's need for a candidate recommendation. Overall, the response is strong but loses a point for minor factual deviations and incomplete coverage of the reference's specific examples.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for test_case in test_cases:\n",
    "    test_query = test_case[\"query\"]\n",
    "    reference = test_case[\"reference_answer\"]\n",
    "\n",
    "    agent_response = await agent.ask(test_query)\n",
    "    evaluation = await evaluate_answer(agent_response, reference)\n",
    "    print(evaluation)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluation-intro (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
