{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6344cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client configuration for Azure OpenAI and Foundry\n",
    "import asyncio\n",
    "\n",
    "from agent_framework.azure import AzureAIAgentClient, AzureOpenAIChatClient\n",
    "from agent_framework.observability import setup_observability \n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.identity.aio import DefaultAzureCredential as AsyncDefaultAzureCredential\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import config\n",
    "\n",
    "load_dotenv()\n",
    "setup_observability()\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "async_credential = AsyncDefaultAzureCredential()    \n",
    "openai_client = AzureOpenAI(\n",
    "    azure_endpoint=config.azure_openai_endpoint,\n",
    "    azure_ad_token_provider=lambda: credential.get_token(\"https://cognitiveservices.azure.com/.default\").token,\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "chat_client = AzureOpenAIChatClient(\n",
    "    credential=credential,\n",
    "    deployment_name=\"gpt-5-chat\",\n",
    "    endpoint=config.azure_openai_endpoint\n",
    ")\n",
    "\n",
    "agent_client = AzureAIAgentClient(\n",
    "    project_endpoint=config.foundry_project_endpoint,\n",
    "    model_deployment_name=\"gpt-5-chat\",\n",
    "    async_credential=async_credential,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab97ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Loaded 3 test cases for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Define test questions and reference answers \n",
    "test_cases = [\n",
    "    {\n",
    "        \"category\": \"Company Information\",\n",
    "        \"query\": \"When was Meridian founded?\",\n",
    "        \"reference_answer\": \"Meridian Strategic Consulting was founded in 2018\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Service Offerings\",\n",
    "        \"query\": \"What specific AI and automation services does Meridian offer, and what kind of ROI can clients expect?\",\n",
    "        \"reference_answer\": \"\"\"Meridian offers comprehensive AI & Automation Implementation services including AI opportunity assessment, solution design and architecture, implementation and testing, and change management support. These engagements typically last 16-28 weeks with teams of 5-10 consultants. Clients can expect a typical ROI of 400-600%, making this one of our highest-value service offerings. Dr. Amanda Foster leads this practice with expertise in machine learning, having 12 patents in ML and automation, and the team has delivered $500M+ in client value through AI implementations.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Expert Identification\", \n",
    "        \"query\": \"I need a consultant with AI/ML expertise and healthcare industry experience for a 6-month project. Who would be the best match?\",\n",
    "        \"reference_answer\": \"\"\"Dr. Amanda Foster would be the ideal match for this project. She is a Senior Partner with expertise in AI/ML and leads both the Digital Transformation practice and Healthcare vertical. She has a PhD in Computer Science from Carnegie Mellon, holds 12 patents in machine learning and automation, and has led AI implementations generating $500M+ in client value. She has specific healthcare experience with projects including AI-powered diagnostic tools, predictive analytics for patient outcomes, and clinical decision support systems. However, she is currently available starting February 2025, so timeline coordination would be needed.\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2542045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for semantic similarity evaluation \n",
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "evaluation_instructions = \"\"\"\n",
    "You are an expert evaluator of AI generated answers.\n",
    "Given a reference answer, evaluation criteria, and an agent-generated response, evaluate the quality of the agent's response based on the following metrics. \n",
    "Provide a score from 1 to 5 for each metric, where 1 is poor and 5 is excellent. Also, provide an overall score with justification.\n",
    "\n",
    "REFERENCE ANSWER:\n",
    "{reference_answer}\n",
    "\n",
    "AGENT RESPONSE:\n",
    "{agent_response}\n",
    "\n",
    "EVALUATION METRICS:\n",
    "1. Accuracy: Are the facts and figures correct?\n",
    "2. Completeness: Does it cover all key points mentioned in the reference answer?\n",
    "3. Clarity: Is the response clear and well-structured?\n",
    "4. Relevance: Does it directly answer the user's question?\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "You MUST return your evaluation as a JSON object with the following structure:\n",
    "{{\n",
    "    \"accuracy\": X,\n",
    "    \"completeness\": X,\n",
    "    \"clarity\": X,\n",
    "    \"relevance\": X,\n",
    "    \"overall\": X,\n",
    "    \"justification\": \"Detailed explanation of your evaluation\"\n",
    "}}\n",
    "\n",
    "Where X is a score from 1-5 (1=Poor, 2=Below Average, 3=Average, 4=Good, 5=Excellent).\n",
    "\"\"\"\n",
    "\n",
    "class EvaluationResponse(BaseModel):\n",
    "    accuracy: int\n",
    "    completeness: int\n",
    "    clarity: int\n",
    "    relevance: int\n",
    "    overall: int\n",
    "    justification: str\n",
    "\n",
    "async def evaluate_answer(response: str, reference: str) -> Any:\n",
    "    prompt = evaluation_instructions.format(reference_answer=reference, agent_response=response)\n",
    "\n",
    "    evaluation = await chat_client.get_response(\n",
    "        [prompt],\n",
    "        model=\"o3-mini\",\n",
    "        temperature=0.0,\n",
    "        tool_choice=\"auto\")\n",
    "\n",
    "    return evaluation.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d27c15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "from main import MeridianKnowledgeBaseAgent\n",
    "\n",
    "agent_instructions = \"\"\"\n",
    "You are a helpful AI assistant. \n",
    "\n",
    "You will receive a question and exerpts of documents returned from a search service in the following format\n",
    "```\n",
    "Question: {question}\n",
    "Exerpts: {context}\n",
    "```\n",
    "\n",
    "Keep thinking untill you have a complete answer. If the question contains multiple parts, split the query into parts and be sure to answer all parts of the question.\n",
    "\n",
    "Use ONLY the provided knowledge base excerpts to answer the user's question as accurately as possible. \n",
    "Do not make assumptions or add information not contained in the excerpts.\n",
    "If you cannot answer (part of the question) state which part of the question you are unable to answer.\n",
    "\"\"\"\n",
    "\n",
    "agent = MeridianKnowledgeBaseAgent(agent_instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c1c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"accuracy\": 5,\n",
      "    \"completeness\": 5,\n",
      "    \"clarity\": 5,\n",
      "    \"relevance\": 5,\n",
      "    \"overall\": 5,\n",
      "    \"justification\": \"The agent's response exactly matches the reference answer, with the correct founding year of Meridian Strategic Consulting. It covers the sole key point from the reference answer, is clear and concise, and directly addresses the question without unnecessary information. Therefore, it scores full marks across all metrics.\"\n",
      "}\n",
      "{\n",
      "    \"accuracy\": 4,\n",
      "    \"completeness\": 4,\n",
      "    \"clarity\": 5,\n",
      "    \"relevance\": 5,\n",
      "    \"overall\": 4,\n",
      "    \"justification\": \"The agent's response is largely accurate, correctly reflecting key facts from the reference answer such as the service offerings, engagement duration (16â€“28 weeks), ROI range (400â€“600%), and $500M+ in client value. However, it introduces additional services (predictive modeling, integration, industry-specific applications) and ROI figures (250â€“500% for predictive models) that are not mentioned in the reference answer, which slightly affects accuracy. Completeness is good, as it covers all major points from the reference answer, including leadership, though it omits Dr. Amanda Foster's name and her patents, which are important details. Clarity is excellentâ€”the response is well-structured with bullet points and clear headings. Relevance is excellent, as the content directly addresses the AI & automation services offered by Meridian. Overall, the response is strong but loses points for omitting the leader's credentials and adding unverified details.\"\n",
      "}\n",
      "{\n",
      "    \"accuracy\": 4,\n",
      "    \"completeness\": 4,\n",
      "    \"clarity\": 5,\n",
      "    \"relevance\": 5,\n",
      "    \"overall\": 4,\n",
      "    \"justification\": \"The agent's response is mostly accurate, correctly identifying Dr. Amanda Foster as the ideal match and including key facts such as her AI/ML expertise, 12 patents, $500M+ in client value, and healthcare experience. However, there are slight deviations from the reference answer: the agent adds details not present in the reference (e.g., 14 years of experience, EHR optimization, HIPAA/FDA compliance, team size of 35 consultants) which may be accurate but are not confirmed in the reference, thus slightly lowering the accuracy score. Completeness is good, as it covers most of the reference points, but it omits her PhD from Carnegie Mellon and specific healthcare projects like AI-powered diagnostic tools and clinical decision support systems. Clarity is excellent, with a well-structured and easy-to-read format. Relevance is excellent, as the response directly addresses the user's request and stays focused on the project match. Overall, the response is strong but loses points for introducing unverified details and omitting some specific reference facts.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for test_case in test_cases:\n",
    "    test_query = test_case[\"query\"]\n",
    "    reference = test_case[\"reference_answer\"]\n",
    "\n",
    "    agent_response = await agent.ask(test_query)\n",
    "    evaluation = await evaluate_answer(agent_response, reference)\n",
    "    print(evaluation)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluation-intro (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
